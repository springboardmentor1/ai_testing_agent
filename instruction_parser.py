from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
import json

# 1.LLM CONFIGURATION (LM Studio - NO API KEY)

llm = ChatOpenAI(
    base_url="http://localhost:1234/v1",  
    api_key="lm-studio",                  
    model="local-model",                  
    temperature=0
)

# 2. STATE DEFINITION

class InstructionState(TypedDict):
    input: str
    output: dict

# 3. PARSER NODE

def parse_instruction(state: InstructionState) -> InstructionState:
    prompt = f"""
You are an AI test case parser.

Convert the following natural language test case into STRICT JSON.
Do NOT add explanations.

JSON format:
{{
  "action": "<short action name>",
  "steps": ["step1", "step2", "..."],
  "assertions": ["assertion1", "assertion2"]
}}

Test case:
{state["input"]}
"""

    response = llm.invoke(prompt)

    # Ensure valid JSON output
    try:
        parsed = json.loads(response.content)
    except json.JSONDecodeError:
        parsed = {
            "error": "Invalid JSON generated by model",
            "raw_output": response.content
        }

    return {
        "input": state["input"],
        "output": parsed
    }

# 4. LANGGRAPH SETUP

graph = StateGraph(InstructionState)
graph.add_node("instruction_parser", parse_instruction)
graph.set_entry_point("instruction_parser")
graph.add_edge("instruction_parser", END)

instruction_parser = graph.compile()

if __name__ == "__main__":
    print("=== Instruction Parser (LM Studio, No API Key) ===\n")
    test_case = input("Enter test case: ")

    result = instruction_parser.invoke({
        "input": test_case
    })

    print("\nParsed Output:")
    print(json.dumps(result["output"], indent=2))
